---
title: "Transformers are Provably Optimal In-context Estimators for Wireless Communications"
collection: publications
category: conferences
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'This paper introduces the concept of in-context estimation (ICE), where pre-trained transformers adapt to new tasks by leveraging limited prompts without explicit optimization. It proves that single-layer softmax attention transformers (SATs) can optimally solve ICE problems for a subclass of cases and demonstrates that multi-layer transformers efficiently handle broader ICE problems, outperforming standard approaches. The study highlights that transformers achieve near-optimal performance with minimal context examples, rivaling estimators with perfect knowledge of the latent context.'
date: 2025-05-03
venue: 'AISTATS'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/abs/2311.00226'
citation: 'Vishnu Teja Kunde, Vicram Rajagopalan, Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Srinivas Shakkottai, Dileep Kalathil, and Jean-Francois Chamberland. "Transformers are Provably Optimal In-context Estimators for Wireless Communications." arXiv preprint, arXiv:2311.00226, 2025'
---

Pre-trained transformers exhibit the capability of adapting to new tasks through in-context learning (ICL), where they efficiently utilize a limited set of prompts without explicit model optimization. The canonical communication problem of estimating transmitted symbols from received observations can be modeled as an in-context learning problem: received observations are a noisy function of transmitted symbols, and this function can be represented by an unknown parameter whose statistics depend on an unknown latent context. This problem, which we term in-context estimation (ICE), has significantly greater complexity than the extensively studied linear regression problem. The optimal solution to the ICE problem is a non-linear function of the underlying context. In this paper, we prove that, for a subclass of such problems, a single-layer softmax attention transformer (SAT) computes the optimal solution of the above estimation problem in the limit of large prompt length. We also prove that the optimal configuration of such a transformer is indeed the minimizer of the corresponding training loss. Further, we empirically demonstrate the proficiency of multi-layer transformers in efficiently solving broader in-context estimation problems. Through extensive simulations, we show that solving ICE problems using transformers significantly outperforms standard approaches. Moreover, just with a few context examples, it achieves the same performance as an estimator with perfect knowledge of the latent context. The code is available [here](https://github.com/vishnutez/in-context-estimation)